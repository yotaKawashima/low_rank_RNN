{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perceptual decision-making\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch_directml\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.mixture import GaussianMixture \n",
    "import dataset.generate_dataset as generate_dataset\n",
    "import models.neural_networks as neural_networks \n",
    "import models.latent_dynamics as latent_dynamics\n",
    "import evaluation.visualise_dynamics as visualise_dynamics\n",
    "import evaluation.visualise_connectivity as visualise_connectivity\n",
    "import matplotlib.colors as mcolors\n",
    "import scipy \n",
    "\n",
    "device = torch_directml.device()\n",
    "print('Using {}'.format(device))\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(2)\n",
    "list_colors = list(mcolors.TABLEAU_COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_trials = batch_size * 250\n",
    "t_onset = 0.1 # 5 / sampling_rate # [s]\n",
    "t_offset = 0.9 # 45 / sampling_rate # [s]\n",
    "t_max = 1500/1000 # [s]\n",
    "sampling_rate = 1/(20/1000) #[Hz]\n",
    "num_trial_to_plot = 5 # should be less than batch_size\n",
    "stimulus_strength_list  = \\\n",
    "    3.2/100 * 2 ** np.arange(0, 5)\n",
    "stimulus_strength_list = \\\n",
    "    np.append(stimulus_strength_list, (-stimulus_strength_list))\n",
    "\n",
    "PDM_dataset_train = generate_dataset.PDMStimulus(num_trials, t_onset, t_offset, t_max, stimulus_strength_list, sampling_rate)\n",
    "PDM_dataset_test = generate_dataset.PDMStimulus(int(num_trials/100), t_onset, t_offset, t_max, stimulus_strength_list, sampling_rate)\n",
    "\n",
    "fig = PDM_dataset_train.plot_stimulus(np.arange(num_trial_to_plot))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(PDM_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(PDM_dataset_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# check a batch \n",
    "for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "    print(i_batch, sample_batched['stimulus'].size(),\n",
    "          sample_batched['label'].size())\n",
    "    \n",
    "    batch_size = len(sample_batched['stimulus'])\n",
    "    \n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 3:\n",
    "        for i_trial in range(num_trial_to_plot):\n",
    "            plt.plot(PDM_dataset_train.time, sample_batched['stimulus'][i_trial, :], \\\n",
    "                    label='trial {:d}, label {:d}'.format(int(i_trial), int(sample_batched['label'][i_trial])))\n",
    "        plt.xlim(0, max(PDM_dataset_train.time))\n",
    "        plt.xlabel('time [s]')\n",
    "        plt.ylabel('u')\n",
    "        plt.legend()\n",
    "        plt.title('Batch {}'.format(i_batch))\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Low rank RNN  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "rank = 1\n",
    "time_step = 20/1000 # s \n",
    "tau = 100/1000 # s\n",
    "model = neural_networks.LowRankRNN(\n",
    "    input_size, hidden_size, \n",
    "    output_size, rank, \n",
    "    time_step, tau).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run simulation & Visualise a time course of each hidden node.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batched = next(iter(train_dataloader))\n",
    "print('try simulation for the first batch')\n",
    "print('shape of stimulus: ', sample_batched['stimulus'].size())\n",
    "print('shape of label:', sample_batched['label'].size())\n",
    "\n",
    "visualise_dynamics.plot_hidden_nodes_single_trial(\n",
    "    device, model, t_max, PDM_dataset_train, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "learning_rate = 5e-3\n",
    "loss_min = 5e-2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "time_step_for_loss = 15 #[-] not in sec    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    num_epoches = len(dataloader)\n",
    "    loss_history = []\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        #print(i_batch, sample_batched['stimulus'].size(),\n",
    "        #      sample_batched['label'].size())      \n",
    "              \n",
    "        _batch_size = len(sample_batched['stimulus'])\n",
    "        hidden_state = model.init_hidden(_batch_size)        \n",
    "        loss = 0 \n",
    "        optimizer.zero_grad()\n",
    "        #print(sample_batched['stimulus'].shape)\n",
    "        num_sample_points = sample_batched['stimulus'].shape[1]\n",
    "        \n",
    "        for i_time in range(num_sample_points):\n",
    "            #print(sample_batched['stimulus'].shape)\n",
    "            input_data = sample_batched['stimulus'][:, i_time].unsqueeze(dim=1).to(device)\n",
    "            label_data = sample_batched['label'].float().to(device)\n",
    "            hidden_state = hidden_state.to(device)\n",
    "            output, hidden_state = model(input_data, hidden_state)\n",
    "            \n",
    "            if i_time >=(num_sample_points - time_step_for_loss):\n",
    "                #print(output.shape)\n",
    "                #print(loss_fn(output.squeeze().to(device), label_data))\n",
    "                loss += loss_fn(output.squeeze().to(device), label_data)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss, current = loss.item(), (i_batch+1) * _batch_size\n",
    "        \n",
    "        if i_batch % 100 == 0:\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{num_epoches*_batch_size:>5d}]\")\n",
    "        \n",
    "        loss_history.append(loss)\n",
    "\n",
    "    return loss_history \n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    num_epoches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0 \n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i_batch, sample_batched in enumerate(dataloader):\n",
    "            _batch_size = len(sample_batched['stimulus'])\n",
    "            hidden_state = model.init_hidden(_batch_size)\n",
    "            num_sample_points = sample_batched['stimulus'].shape[1]\n",
    "         \n",
    "            for i_time in range(num_sample_points):\n",
    "                input_data = sample_batched['stimulus'][:, i_time].unsqueeze(dim=1).to(device)\n",
    "                label_data = sample_batched['label'].float().to(device)\n",
    "                hidden_state = hidden_state.to(device)\n",
    "                output, hidden_state = model(input_data, hidden_state)   \n",
    "             \n",
    "            if i_time >=(num_sample_points - time_step_for_loss):\n",
    "                test_loss += loss_fn(output.squeeze().to(device), label_data)\n",
    "\n",
    "            correct += ((output.sign() * label_data) == 1).sum().item() / _batch_size\n",
    "\n",
    "    test_loss /= num_epoches\n",
    "    correct /= num_epoches\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model \n",
    "loss_history = train(train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel('batch [-]')\n",
    "plt.ylabel('error [-]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model \n",
    "test(test_dataloader, model, loss_fn)\n",
    "\n",
    "# visualise dynamics of trained RNN\n",
    "visualise_dynamics.plot_dynamics_each_trial(\n",
    "    device, model, t_max, PDM_dataset_test, test_dataloader, num_trial_to_plot=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis of selectivity and connectivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute covariance matrix in connectivity space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = model.feedforward_input.weight.cpu().detach().numpy()\n",
    "readout_vector = model.readout.weight.cpu().detach().numpy().T\n",
    "input_selection_vector = model.recurrent_input.right_singular_vector.cpu().detach().numpy().T\n",
    "output_vector = model.recurrent_input.left_singular_vector.cpu().detach().numpy()\n",
    "\n",
    "# set connectivity data and compute cov matrix\n",
    "connectivity_data = np.hstack((input_selection_vector, output_vector, input_vector, readout_vector))\n",
    "connectivity_covariance_matrix = np.cov(connectivity_data.T)\n",
    "vector_names = [rf'n$^{i}$' for i in range(1, model.rank+ 1)]\n",
    "vector_names.extend([rf'm$^{i}$' for i in range(1, model.rank+ 1)])\n",
    "vector_names.extend(['I', 'W'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot neurons in connectivity space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_connectivity.plot_neurons_connectivity_space(\n",
    "    connectivity_data, connectivity_covariance_matrix, model.rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim = [-2.2, 2.2]\n",
    "visualise_connectivity.plot_connectivity_covariance_matrix(\n",
    "    connectivity_data, connectivity_covariance_matrix, model.rank, clim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gaussian mixture model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate neurons in the network by a single-population GMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit to 4-d gaussian distribution with one population\n",
    "GMM = GaussianMixture(n_components = 1, random_state=0)\n",
    "GMM.fit(connectivity_data)\n",
    "\n",
    "for i in range(connectivity_data.shape[1]):\n",
    "    print(vector_names[i] + rf' mean: {GMM.means_[0][i]}')\n",
    "    #GMM.covariances_\n",
    "\n",
    "# resample from the fitted distribution\n",
    "resampled_connectivity_data = \\\n",
    "    np.random.multivariate_normal(GMM.means_[0], GMM.covariances_[0], size=hidden_size)\n",
    "print(f'shape of resampled_connectivity_data: {resampled_connectivity_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise original neurons and re-sampled neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_connectivity.plot_resampled_neurons_connectivity_space(\n",
    "    resampled_connectivity_data, connectivity_data, connectivity_covariance_matrix, model.rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the network with the resampled connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a network with the resampled connecitivity data.\n",
    "resampled_model = \\\n",
    "    neural_networks.LowRankRNN(input_size, hidden_size, output_size, rank, time_step, tau).to(device)\n",
    "\n",
    "# use resampled data for the model \n",
    "# input vector (hiddeni_size x input_size)\n",
    "resampled_model.feedforward_input.weight = \\\n",
    "    torch.Tensor(resampled_connectivity_data[:, -2, np.newaxis])\n",
    "# readout vector (output_size x hidden_size)\n",
    "resampled_model.readout.weight = \\\n",
    "    torch.Tensor(resampled_connectivity_data[:, -1, np.newaxis].T)\n",
    "# input selection vectors (rank x hidden_size)\n",
    "resampled_model.recurrent_input.right_singular_vector = \\\n",
    "    torch.nn.Parameter(torch.Tensor(resampled_connectivity_data[:, 0:rank].T))\n",
    "# output vectors (hidden_size x rank)\n",
    "resampled_model.recurrent_input.left_singular_vector = \\\n",
    "    torch.nn.Parameter(torch.Tensor(resampled_connectivity_data[:, rank:-2]))\n",
    "\n",
    "# test the resampled model\n",
    "test(test_dataloader, resampled_model, loss_fn)\n",
    "visualise_dynamics.plot_dynamics_each_trial(\n",
    "    device, resampled_model, t_max, PDM_dataset_test, test_dataloader, num_trial_to_plot=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Project data onto the m-I subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batched = next(iter(test_dataloader))\n",
    "# run feedforward simulation and obtain the history of hidden state\n",
    "hidden_state_history = np.empty((PDM_dataset_test.num_sample_points, batch_size, hidden_size))\n",
    "for i_time in range(PDM_dataset_test.num_sample_points):\n",
    "      if i_time == 0:\n",
    "            hidden_state = model.init_hidden(batch_size)\n",
    "      input_data = sample_batched['stimulus'][:, i_time].unsqueeze(dim=1)\n",
    "      with torch.no_grad():\n",
    "            output, hidden_state = model(input_data.to(device), hidden_state.to(device))\n",
    "      hidden_state_history[i_time, :, :] = hidden_state.cpu().detach().numpy()\n",
    "\n",
    "# project data to m \n",
    "projection_m = (hidden_state_history @ output_vector).squeeze()\n",
    "# project data to I\n",
    "projection_I = (hidden_state_history @ input_vector).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot trajectory of hidden state in the m-I plane\n",
    "plt.scatter(projection_m[0, 0], projection_I[0, 0], s=50, color='k', label='start', zorder=10)\n",
    "for i_trial in range(num_trial_to_plot):\n",
    "    plt.plot(projection_m[:, i_trial], \n",
    "             projection_I[:, i_trial], \n",
    "             color=list_colors[i_trial],\n",
    "             label=f'trials {i_trial}')\n",
    "    plt.scatter(projection_m[-1, i_trial], \n",
    "                projection_I[-1, i_trial], \n",
    "                s=50, c=list_colors[i_trial], label='end', zorder=10)\n",
    "\n",
    "plt.xlabel('latent variable k [-]')\n",
    "plt.ylabel('latent variable u [-]')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dynamics with latent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_model = latent_dynamics.LatentDynamics(rank, connectivity_covariance_matrix, time_step)\n",
    "\n",
    "# run simulation using the first batch from test_dataloader \n",
    "i_batch = 0\n",
    "sample_batched = next(iter(test_dataloader))\n",
    "stimulus_batched = sample_batched['stimulus'].numpy()\n",
    "label_batched = sample_batched['label'].numpy()\n",
    "latent_model.run_simulation(stimulus_batched, label_batched)\n",
    "correct = np.sum(label_batched == np.sign(latent_model.ks_history[:,0,-1])) / len(label_batched)\n",
    "print(f\"Latent dynamics: \\n Accuracy: {(100*correct):>0.1f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_dynamics.plot_latent_dynamics_each_trial_PDM(\n",
    "    PDM_dataset_test.time, latent_model.ks_history, \n",
    "    stimulus_batched, label_batched, \n",
    "    i_batch, num_trial_to_plot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyse locally linear dynamics around fixed points of a trained RNN\n",
    "Find fixed points by minimising enery "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
